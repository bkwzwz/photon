
From a663a860819287e041c3de672aad1d8543098ecc Mon Sep 17 00:00:00 2001
From: Jonathon Jongsma <jjongsma@redhat.com>
Date: Thu, 5 Dec 2019 10:08:52 -0600
Subject: [PATCH] qemu: don't hold both jobs for suspend
We have to assume that the guest agent may be malicious so we don't want
to allow any agent queries to block any other libvirt API. By holding a
monitor job while we're querying the agent, we open ourselves up to a
DoS.
So split the function up a bit to only hold the monitor job while
querying qemu for whether the domain supports suspend. Then acquire only
an agent job while issuing the agent suspend command.

Signed-off-by: Jonathon Jongsma <jjongsma@redhat.com>
Signed-off-by: Michal Privoznik <mprivozn@redhat.com>
Reviewed-by: Michal Privoznik <mprivozn@redhat.com>

Re-created patch for libvirt-4.7.0 version by referring actual upstream patch.
Signed-off-by: Harinadh Dommaraju <hdommaraju@vmware.com>
Reviewed-by: Keerthana Kalyanasundaram <keerthanak@vmware.com>
---

--- a/src/qemu/qemu_driver.c	2020-04-03 01:57:05.418560670 +0530
+++ b/src/qemu/qemu_driver.c	2020-04-06 00:07:06.201512683 +0530
@@ -18941,6 +18941,72 @@ qemuDomainGetCPUStats(virDomainPtr domai
     return ret;
 }
 
+
+static int
+qemuDomainProbeQMPCurrentMachine(virQEMUDriverPtr driver,virDomainObjPtr vm,bool *wakeupSupported)
+{
+    qemuDomainObjPrivatePtr priv = vm->privateData;
+    qemuMonitorCurrentMachineInfo info = { 0 };
+    int rv;
+
+    qemuDomainObjEnterMonitor(driver, vm);
+    rv = qemuMonitorGetCurrentMachineInfo(priv->mon, &info);
+    if (qemuDomainObjExitMonitor(driver, vm) < 0 || rv < 0)
+        return -1;
+
+    *wakeupSupported = info.wakeupSuspendSupport;
+    return 0;
+}
+
+
+/* returns -1 on error, or if query is not supported, 0 if query was
+ * successful*/
+
+
+static int
+qemuDomainQueryWakeupSuspendSupport(virQEMUDriverPtr driver,virDomainObjPtr vm,bool *wakeupSupported)
+{   
+    qemuDomainObjPrivatePtr priv = vm->privateData;
+    int ret = -1;
+                                
+    if (!virQEMUCapsGet(priv->qemuCaps, QEMU_CAPS_QUERY_CURRENT_MACHINE))
+        return -1;
+    if (qemuDomainObjBeginJob(driver, vm, QEMU_JOB_MODIFY) < 0)
+        return -1;
+    if ((ret = virDomainObjCheckActive(vm)) < 0)
+        goto endjob;
+    ret = qemuDomainProbeQMPCurrentMachine(driver, vm, wakeupSupported);                            
+endjob:
+    qemuDomainObjEndJob(driver, vm);
+    return ret;
+}
+
+static int
+qemuDomainPMSuspendAgent(virQEMUDriverPtr driver,virDomainObjPtr vm,unsigned int target)
+{   
+    qemuAgentPtr agent;
+    int ret = -1;
+                            
+    if (qemuDomainObjBeginAgentJob(driver, vm, QEMU_AGENT_JOB_MODIFY) < 0)
+        return -1;
+                                    
+    if ((ret = virDomainObjCheckActive(vm)) < 0)
+        goto endjob;
+                            
+    if (!qemuDomainAgentAvailable(vm, true))
+        goto endjob;
+                            
+    agent = qemuDomainObjEnterAgent(vm);
+    ret = qemuAgentSuspend(agent, target);
+    qemuDomainObjExitAgent(vm, agent);
+                                        
+endjob:
+    qemuDomainObjEndAgentJob(vm);
+    return ret;
+}
+
+
+
 static int
 qemuDomainPMSuspendForDuration(virDomainPtr dom,
                                unsigned int target,
@@ -18949,8 +19015,8 @@ qemuDomainPMSuspendForDuration(virDomain
 {
     virQEMUDriverPtr driver = dom->conn->privateData;
     virDomainObjPtr vm;
-    qemuAgentPtr agent;
     int ret = -1;
+    bool wakeupSupported;
 
     virCheckFlags(0, -1);
 
@@ -18975,38 +19041,38 @@ qemuDomainPMSuspendForDuration(virDomain
     if (virDomainPMSuspendForDurationEnsureACL(dom->conn, vm->def) < 0)
         goto cleanup;
 
-    if (qemuDomainObjBeginAgentJob(driver, vm, QEMU_AGENT_JOB_MODIFY) < 0)
-        goto cleanup;
-
-    if (virDomainObjCheckActive(vm) < 0)
-        goto endjob;
-
+    /*
+     *The case we want to handle here is when QEMU has the API (i.e
+     *QEMU_CAPS_QUERY_CURRENT_MACHINE is set). Otherwise, do not interfere
+     *with the suspend process. This means that existing running domains,
+     *that don't know about this cap, will keep their old behavior of
+     *suspending 'in the dark'.
+     */
+    if (qemuDomainQueryWakeupSuspendSupport(driver, vm, &wakeupSupported) == 0) {
+        if (!wakeupSupported) {
+            virReportError(VIR_ERR_OPERATION_UNSUPPORTED, "%s",_("Domain does not have suspend support"));
+                goto cleanup;
+        }
+    }
     if (vm->def->pm.s3 || vm->def->pm.s4) {
         if (vm->def->pm.s3 == VIR_TRISTATE_BOOL_NO &&
             (target == VIR_NODE_SUSPEND_TARGET_MEM ||
              target == VIR_NODE_SUSPEND_TARGET_HYBRID)) {
             virReportError(VIR_ERR_INTERNAL_ERROR, "%s",
                            _("S3 state is disabled for this domain"));
-            goto endjob;
+            goto cleanup;
         }
 
         if (vm->def->pm.s4 == VIR_TRISTATE_BOOL_NO &&
             target == VIR_NODE_SUSPEND_TARGET_DISK) {
             virReportError(VIR_ERR_INTERNAL_ERROR, "%s",
                            _("S4 state is disabled for this domain"));
-            goto endjob;
+            goto cleanup;
         }
     }
 
-    if (!qemuDomainAgentAvailable(vm, true))
-        goto endjob;
-
-    agent = qemuDomainObjEnterAgent(vm);
-    ret = qemuAgentSuspend(agent, target);
-    qemuDomainObjExitAgent(vm, agent);
+    ret = qemuDomainPMSuspendAgent(driver, vm, target);
 
- endjob:
-    qemuDomainObjEndAgentJob(vm);
 
  cleanup:
     virDomainObjEndAPI(&vm);
--- a/src/qemu/qemu_capabilities.c	2020-04-03 15:59:41.427949221 +0530
+++ b/src/qemu/qemu_capabilities.c	2020-04-06 02:25:08.701084854 +0530
@@ -508,6 +508,7 @@ VIR_ENUM_IMPL(virQEMUCaps, QEMU_CAPS_LAS
               /* 315 */
               "vfio-pci.display",
               "blockdev",
+              "query-current-machine",
     );
 
 
@@ -1022,6 +1023,7 @@ struct virQEMUCapsStringFlags virQEMUCap
     { "query-cpus-fast", QEMU_CAPS_QUERY_CPUS_FAST },
     { "qom-list-properties", QEMU_CAPS_QOM_LIST_PROPERTIES },
     { "blockdev-del", QEMU_CAPS_BLOCKDEV_DEL },
+    { "query-current-machine", QEMU_CAPS_QUERY_CURRENT_MACHINE },
 };
 
 struct virQEMUCapsStringFlags virQEMUCapsMigration[] = {
--- a/src/qemu/qemu_capabilities.h	2020-04-03 15:59:53.219948612 +0530
+++ b/src/qemu/qemu_capabilities.h	2020-04-03 15:45:22.255993601 +0530
@@ -492,6 +492,8 @@ typedef enum { /* virQEMUCapsFlags group
     /* 315 */
     QEMU_CAPS_VFIO_PCI_DISPLAY, /* -device vfio-pci.display */
     QEMU_CAPS_BLOCKDEV, /* -blockdev and blockdev-add are supported */
+    QEMU_CAPS_QUERY_CURRENT_MACHINE, /* query-current-machine command */
+
 
     QEMU_CAPS_LAST /* this must always be the last item */
 } virQEMUCapsFlags;
--- a/src/qemu/qemu_monitor.c	2020-04-06 02:40:31.633037180 +0530
+++ b/src/qemu/qemu_monitor.c	2020-04-06 02:42:57.869029626 +0530
@@ -4429,3 +4429,12 @@ qemuMonitorGetPRManagerInfo(qemuMonitorP
     virHashFree(info);
     return ret;
 }
+
+int
+qemuMonitorGetCurrentMachineInfo(qemuMonitorPtr mon,
+        qemuMonitorCurrentMachineInfoPtr info)
+{
+        QEMU_CHECK_MONITOR(mon);
+
+        return qemuMonitorJSONGetCurrentMachineInfo(mon, info);
+}
--- a/src/qemu/qemu_monitor.h	2020-04-06 00:19:31.021474209 +0530
+++ b/src/qemu/qemu_monitor.h	2020-04-06 00:26:00.901454070 +0530
@@ -1187,5 +1187,12 @@ struct _qemuMonitorPRManagerInfo {
 
 int qemuMonitorGetPRManagerInfo(qemuMonitorPtr mon,
                                 virHashTablePtr *retinfo);
+typedef struct  _qemuMonitorCurrentMachineInfo qemuMonitorCurrentMachineInfo;
+typedef qemuMonitorCurrentMachineInfo *qemuMonitorCurrentMachineInfoPtr;
+struct _qemuMonitorCurrentMachineInfo {
+        bool wakeupSuspendSupport;
+};
+
+int qemuMonitorGetCurrentMachineInfo(qemuMonitorPtr mon,qemuMonitorCurrentMachineInfoPtr info);
 
 #endif /* QEMU_MONITOR_H */
--- a/src/qemu/qemu_monitor_json.c	2020-04-06 00:59:00.621351809 +0530
+++ b/src/qemu/qemu_monitor_json.c	2020-04-06 01:15:21.089301163 +0530
@@ -8396,3 +8396,49 @@ qemuMonitorJSONGetPRManagerInfo(qemuMoni
     return ret;
 
 }
+
+static int
+qemuMonitorJSONExtractCurrentMachineInfo(virJSONValuePtr reply,
+        qemuMonitorCurrentMachineInfoPtr info)
+{   
+    virJSONValuePtr data;
+            
+    data = virJSONValueObjectGetObject(reply, "return");
+    if (!data)
+        goto malformed;
+                    
+    if (virJSONValueObjectGetBoolean(data, "wakeup-suspend-support",&info->wakeupSuspendSupport) < 0)
+        goto malformed;
+                        
+    return 0;
+                         
+    malformed:
+        virReportError(VIR_ERR_INTERNAL_ERROR, "%s",_("malformed qemu-current-machine reply"));
+        return -1;
+}
+
+
+int
+qemuMonitorJSONGetCurrentMachineInfo(qemuMonitorPtr mon,
+        qemuMonitorCurrentMachineInfoPtr info)
+{
+    int ret = -1;
+    virJSONValuePtr cmd;
+    virJSONValuePtr reply = NULL;
+
+    if (!(cmd = qemuMonitorJSONMakeCommand("query-current-machine",NULL)))
+        return -1;
+
+    if (qemuMonitorJSONCommand(mon, cmd, &reply) < 0)
+        goto cleanup;
+
+    if (qemuMonitorJSONCheckReply(cmd, reply, VIR_JSON_TYPE_OBJECT) < 0)
+        goto cleanup;
+
+    ret = qemuMonitorJSONExtractCurrentMachineInfo(reply, info);
+
+    cleanup:
+        virJSONValueFree(cmd);
+        virJSONValueFree(reply);
+        return ret;
+}
--- a/src/qemu/qemu_monitor_json.h	2020-04-06 00:58:44.033352666 +0530
+++ b/src/qemu/qemu_monitor_json.h	2020-04-06 01:02:34.785340746 +0530
@@ -576,5 +576,7 @@ int qemuMonitorJSONBlockdevMediumInsert(
 int qemuMonitorJSONGetPRManagerInfo(qemuMonitorPtr mon,
                                     virHashTablePtr info)
     ATTRIBUTE_NONNULL(1) ATTRIBUTE_NONNULL(2);
+int qemuMonitorJSONGetCurrentMachineInfo(qemuMonitorPtr mon,qemuMonitorCurrentMachineInfoPtr info)
+    ATTRIBUTE_NONNULL(1) ATTRIBUTE_NONNULL(2);
 
 #endif /* QEMU_MONITOR_JSON_H */
